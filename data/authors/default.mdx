---
name: Zhaorui Yang
avatar: /static/images/spongebob.jpg
occupation: Ph.D. student
company: Zhejiang University
email: zhaorui.yang@zju.edu.cn
github: https://github.com/rickyang1114
homepage: https://rickyang1114.github.io
googlescholar: https://scholar.google.com/citations?user=AGjSco8AAAAJ
---

Link to the paper: [arxiv](https://arxiv.org)

**Visualizations** play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the **automated generation of interleaved texts and visualizations** underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports.

<img src="/mdr/framework.png" alt="framework" className="h-full w-full" />

To address these challenges, we propose **Formal Description of Visualization (FDV)**, a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce **Multimodal DeepResearcher**, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation.
